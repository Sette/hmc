import pandas as pd
import numpy as np
import tensorflow as tf
import json
import ast
import os
import csv
import math
from sklearn.utils import shuffle
from math import ceil
from sklearn.model_selection import train_test_split
from tqdm.notebook import tqdm


tqdm.pandas()



args = pd.Series({
    "root_dir":"/mnt/disks/data/",
    "dataset_path":"/mnt/disks/data/fma/fma_large",
    "embeddings":"music_style",
    "sequence_size": 1280,
    "train_id": "hierarchical_all",
    'sample_size': 1
})




base_path = "/mnt/disks/data/fma/trains"


job_path = os.path.join(base_path,args.train_id)


tfrecord_path = os.path.join(job_path,"tfrecords")

# In[16]:

base_path = os.path.join(args.root_dir,"fma")

# In[17]:

models_path = os.path.join(args.root_dir,"models")


metadata_path_fma = os.path.join(base_path,"fma_metadata")

# In[18]:

metadata_path = os.path.join(job_path,"metadata.json")


categories_labels_path = os.path.join(job_path,"labels.json")





def __load_json__(path):
    with open(path, 'r') as f:
        tmp = json.loads(f.read())

    return tmp






def create_dir(path):
    # checking if the directory demo_folder2 
    # exist or not.
    if not os.path.isdir(path):

        # if the demo_folder2 directory is 
        # not present then create it.
        os.makedirs(path)
    return True




import shutil
shutil.rmtree(job_path)


create_dir(job_path)


tracks = os.path.join(metadata_path_fma,"tracks_valid.csv")


df_tracks = pd.read_csv(tracks)


df_tracks


df_tracks["full_genre_id"] = df_tracks.full_genre_id.apply(lambda x : ast.literal_eval(x))


df_tracks = df_tracks.sample(frac=args.sample_size)


df_tracks.full_genre_id.value_counts()


# Remover linhas com genre_id vazio
df_tracks = df_tracks[df_tracks['full_genre_id'].map(len) > 0]



df_tracks.full_genre_id.value_counts()


def get_label_size(labels):
    return max([len(label) for label in labels])


labels_size = df_tracks.full_genre_id.apply(lambda x: get_label_size(x))


labels_size


labels_size.unique()


labels_size = max(labels_size)


labels_size


### Function for parse label to sctructure of hierarhical scheme

def parse_label(label,label_size):
    # label = label.split('-')
    # preencher com 0 no caso de haver menos de 5 níveis
    labels = np.zeros(label_size,dtype=int)
    for i, label in enumerate(label):
        if i == 5:
            break
        # Aqui você pode fazer a conversão do label em um índice inteiro usando um dicionário ou outro método
        # Neste exemplo, estou apenas usando a posição da label na lista como índice
        labels[i] = label
    return labels


def parse_labels(labels,label_size=5):
    cv_labels = []
    for label in labels:
        cv_labels.append(parse_label(label,label_size))
    return cv_labels
    


parsed_labels = df_tracks.full_genre_id.apply(lambda x: parse_labels(x))


parsed_labels


df_tracks['full_genre_id']


def convert_label(label,level):
    # return label[:level]
    return '-'.join([str(value) for value in label[:level]])

def convert_labels(labels, level):
    cv_labels = []
    for label in labels:
        cv_labels.append(convert_label(label,level))
    return cv_labels
        


df_tracks['labels_1'] = parsed_labels.progress_apply(lambda x: convert_labels(x,level=1))
df_tracks['labels_2'] = parsed_labels.progress_apply(lambda x: convert_labels(x,level=2))
df_tracks['labels_3'] = parsed_labels.progress_apply(lambda x: convert_labels(x,level=3))
df_tracks['labels_4'] = parsed_labels.progress_apply(lambda x: convert_labels(x,level=4))
df_tracks['labels_5'] = parsed_labels.progress_apply(lambda x: convert_labels(x,level=5))





df_tracks.rename(columns={'track_id_':'track_id'}, inplace=True)


df_tracks.fu


def get_unique_labels(all_labels):
    df = pd.DataFrame({'labels': []})
    cv_labels = []
    
    for labels in all_labels:
        if len(labels) > 1:
            cv_labels.append(labels[0])
            for label in labels:
                cv_labels.append(label)
                
    df['labels'] = cv_labels
    
    return df.labels.unique()


categories_df = pd.DataFrame({'level5': get_unique_labels(df_tracks.labels_5.values)})


categories_df


categories_df['level1'] = categories_df.level5.progress_apply(lambda x: '-'.join(x.split('-')[:1]))
categories_df['level2'] = categories_df.level5.progress_apply(lambda x: '-'.join(x.split('-')[:2]))
categories_df['level3'] = categories_df.level5.progress_apply(lambda x: '-'.join(x.split('-')[:3]))
categories_df['level4'] = categories_df.level5.progress_apply(lambda x: '-'.join(x.split('-')[:4]))


categories_df


genres_df = pd.read_csv(os.path.join(metadata_path_fma,'genres.csv'))



def get_labels_name(x, genres_df):
    full_name = []
    genre_root = ""
    for genre in x.split('-'):
        genre_df = genres_df[genres_df['genre_id'] == int(genre)]
        if genre_df.empty:
            genre_name = genre_root 
        else:
            genre_name = genre_df.title.values.tolist()[0]
            genre_root = genre_name
        full_name.append(genre_name)
    full_name = '>'.join(full_name)
    return full_name
    # return genres_df[genres_df['genre_id'] == int(x)].title.values.tolist()[0]


categories_df['level5_name'] = categories_df.level5.apply(lambda x: get_labels_name(x,genres_df))


categories_df


def __create_labels__(categories_df):
    data = {
        "label1": {},
        "label2": {},
        "label3": {},
        "label4": {},
        "label5": {},
        "label1_inverse": [],
        "label2_inverse": [],
        "label3_inverse": [],
        "label4_inverse": [],
        "label5_inverse": [],
        "label1_name": {},
        "label2_name": {},
        "label3_name": {},
        "label4_name": {},
        "label5_name": {},
    }

    idx = 0
    for id_x, cat in enumerate(set(categories_df.level1.values.tolist())):
        data['label1'][cat] = idx
        data['label1_inverse'].append(cat)
        data['label1_count'] = idx + 1
        idx += 1

    for id_x, cat in enumerate(set(categories_df.level2.values.tolist())):
        data['label2'][cat] = idx
        data['label2_inverse'].append(cat)
        data['label2_count'] = idx + 1
        idx += 1
    for id_x, cat in enumerate(set(categories_df.level3.values.tolist())):
        data['label3'][cat] = idx
        data['label3_inverse'].append(cat)
        data['label3_count'] = idx + 1
        idx += 1

    for id_x, cat in enumerate(set(categories_df.level4.values.tolist())):
        data['label4'][cat] = idx
        data['label4_inverse'].append(cat)
        data['label4_count'] = idx + 1
        idx += 1
    for idx, cat in enumerate(set(categories_df.level5.values.tolist())):
        data['label5'][cat] = idx
        data['label5_inverse'].append(cat)
        data['label5_count'] = idx + 1
        idx += 1
    for cat5, cat1, cat2, cat3, cat4, name5 in categories_df.values:
        name1 = '>'.join(name5.split('>')[:1])
        name2 = '>'.join(name5.split('>')[:2])
        name3 = '>'.join(name5.split('>')[:3])
        name4 = '>'.join(name5.split('>')[:4])
        
        data['label1_name'][cat1] = name1
        data['label2_name'][cat2] = name2
        data['label3_name'][cat3] = name3
        data['label4_name'][cat4] = name4
        data['label5_name'][cat5] = name5
    return data


with open(categories_labels_path, 'w+') as f:
    f.write(json.dumps(__create_labels__(categories_df)))


labels =__create_labels__(categories_df)


labels['label1']


labels['label1_count']


def parse_tfr_element(element):
    #use the same structure as above; it's kinda an outline of the structure we now want to create
    data = {
        'emb' : tf.io.FixedLenFeature([], tf.string),
        'track_id' : tf.io.FixedLenFeature([], tf.int64),
    }
    
    content = tf.io.parse_single_example(element, data)

    track_id = content['track_id']
    emb = content['emb']
    

    #get our 'feature'-- our image -- and reshape it appropriately
    feature = tf.io.parse_tensor(emb, out_type=tf.float32)
    return (feature, track_id)


def get_dataset(filename):
    #create the dataset
    dataset = tf.data.TFRecordDataset(filename)

    #pass every single feature through our mapping function
    dataset = dataset.map(
        parse_tfr_element
    )

    return dataset





import numpy as np


def load_dataset(path,dataset=args.embeddings):
    tfrecords_path = os.path.join(path,'tfrecords',dataset)
    tfrecords_path = [os.path.join(tfrecords_path, path) for path in os.listdir(tfrecords_path)]
    dataset = get_dataset(tfrecords_path)
    df = pd.DataFrame(
        dataset.as_numpy_iterator(),
        columns=['feature', 'track_id']
    )

    df.dropna(inplace=True)
    
    try:
        df.feature = df.feature.apply(lambda x: x[0] if x.shape[0] != 0 else None)
    except:
        print(x)
    return df



def __split_data__(group, percentage=0.1):
    if len(group) == 1:
        return group, group

    shuffled = shuffle(group.values)
    finish_test = int(ceil(len(group) * percentage))

    first = pd.DataFrame(shuffled[:finish_test], columns=group.columns)
    second = pd.DataFrame(shuffled[finish_test:], columns=group.columns)

    return first, second

def get_labels(labels_dict, labels, level='label1'):
    cv_labels = []
    for label in labels:
        cv_labels.append(labels_dict[level][str(label)])
    return cv_labels
        


def select_dataset(df_tracks, labels_dict):
    
#     dataset_testset_path = os.path.join(tfrecord_path,'test')
#     dataset_validationset_path = os.path.join(tfrecord_path,'val')
#     dataset_trainset_path = os.path.join(tfrecord_path,'train')
    
    df = load_dataset(args.dataset_path, dataset=args.embeddings)

    df.dropna(inplace=True)

    df_tracks = df_tracks.merge(df, on='track_id')

    df_tracks.loc[:,'labels_1'] = df_tracks.labels_1.progress_apply(lambda x: get_labels(labels_dict, x, level='label1'))
    df_tracks.loc[:,'labels_2'] = df_tracks.labels_2.progress_apply(lambda x: get_labels(labels_dict, x, level='label2'))
    df_tracks.loc[:,'labels_3'] = df_tracks.labels_3.progress_apply(lambda x: get_labels(labels_dict, x, level='label3'))
    df_tracks.loc[:,'labels_4'] = df_tracks.labels_4.progress_apply(lambda x: get_labels(labels_dict, x, level='label4'))
    df_tracks.loc[:,'labels_5'] = df_tracks.labels_5.progress_apply(lambda x: get_labels(labels_dict, x, level='label5'))

    tests = []
    trains = []
    validations = []
    groups = df_tracks.groupby("labels_5")


    count = 0
    items_count = 0
    total = len(groups)
    total_items = len(df_tracks)
    oversampling_size = 30  # int(group_sizes.mean() + group_sizes.std() * 2)
    print(f"oversampling_size: {oversampling_size}")

    for code, group in groups:
        test, train_to_split = __split_data__(group, 0.01)  # 10%
        train_to_split = train_to_split
        validation, train = __split_data__(train_to_split, 0.01)  # %1

        tests.append(test)
        validations.append(validation)

        ## this increase the numner of samples when classes has low quantity
        count_train = len(train)
        if count_train < oversampling_size:
            train = train.sample(oversampling_size, replace=True)

        trains.append(train)

        count += 1
        items_count += count_train
        
    df_test = pd.concat(tests, sort=False).sample(frac=1).reset_index(drop=True)
    # .to_csv(dataset_testset_path, index=False,quoting=csv.QUOTE_ALL)
    df_val = pd.concat(validations, sort=False).sample(frac=1).reset_index(drop=True)
    df_train = pd.concat(trains, sort=False).sample(frac=1).reset_index(drop=True)

    return df_train, df_test, df_val


df_train, df_test, df_val = select_dataset(df_tracks, labels)


df_train


def _bytes_feature(value):
  ### Returns a bytes_list from a string / byte."""
    if isinstance(value, type(tf.constant(0))): # if value ist tensor
        value = value.numpy() # get value of tensor
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))

def _float_feature(value):
  ### Returns a floast_list from a float / double."""
    return tf.train.Feature(float_list=tf.train.FloatList(value=value))

def _int64List_feature(value):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))

def _int64_feature(value):
  ###  Returns an int64_list from a bool / enum / int / uint."""
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

def serialize_array(array):
    array = tf.io.serialize_tensor(array)
    return array


def parse_single_music(data,labels):
    track_id, _, cat1, cat2, cat3, cat4, cat5, music = data
    
    
    label1 = np.array([cat1, labels['label1_count']], np.int64)
    label2 = np.array([cat2, labels['label2_count']], np.int64)
    label3 = np.array([cat3, labels['label3_count']], np.int64)
    label4 = np.array([cat4, labels['label4_count']], np.int64)
    label5 = np.array([cat5, labels['label5_count']], np.int64)
    
    
    
    #define the dictionary -- the structure -- of our single example
    data = {
        'label1': _int64List_feature(label1),
        'label2': _int64List_feature(label2),
        'label3': _int64List_feature(label3),
        'label4': _int64List_feature(label4),
        'label5': _int64List_feature(label5),
        # 'features' : _bytes_feature(serialize_array(music)),
        'features' : _float_feature(music),
        'track_id' : _int64_feature(track_id)
    }
    #create an Example, wrapping the single features
    out = tf.train.Example(features=tf.train.Features(feature=data))

    return out


def generate_tf_record(df,tf_path='val'):
    create_dir(tf_path)
    
    
    batch_size = 1024 * 50 # 50k records from each file batch
    count = 0
    total = math.ceil(len(df) / batch_size)

    for i in range(0, len(df), batch_size):
        batch_df = df[i:i+batch_size]
        
        tfrecords = [parse_single_music(data, labels) for data in batch_df.values]
        
        path = f"{tf_path}/{str(count).zfill(10)}.tfrecord"
        
        #with tf.python_io.TFRecordWriter(path) as writer:
        with tf.io.TFRecordWriter(path) as writer:
            for tfrecord in tfrecords:
                writer.write(tfrecord.SerializeToString())

        print(f"{count} {len(tfrecords)} {path}")
        count += 1
        print(f"{count}/{total} batchs / {count * batch_size} processed")

    print(f"{count}/{total} batchs / {len(df)} processed")
    
    return tf_path

    


tfrecord_path


val_path = generate_tf_record(df_val,tf_path=os.path.join(tfrecord_path,'val'))


test_path = generate_tf_record(df_test,tf_path=os.path.join(tfrecord_path,'test'))


train_path = generate_tf_record(df_train,tf_path=os.path.join(tfrecord_path,'train'))


def create_metadata(metadata_path):

    with open(metadata_path, 'w+') as f:
        f.write(json.dumps({
            'sequence_size': args.sequence_size,
            'n_levels': labels_size,
            'labels_size': [labels['label1_count'],labels['label2_count'],
                           labels['label3_count'],labels['label4_count'],
                           labels['label5_count']],
            'val_path': val_path,
            'train_path': train_path,
            'test_path': test_path,
            'trainset_count': len(df_train),
            'validationset_count': len(df_val),
            'testset_count': len(df_test)
        }))


create_metadata(metadata_path)


job_path


df_tracks.to_csv(os.path.join(job_path,"tracks.csv"),index=False)


with open(categories_labels_path, 'r') as f:
    labels = json.loads(f.read())


levels_size = {'level1_size': labels['label1_count']-1,
        'level2_size': labels['label2_count']-1,
        'level3_size': labels['label3_count']-1,
        'level4_size': labels['label4_count']-1,
        'level5_size': labels['label5_count']-1}


levels_size['level1_size']
