{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d9fb572-bb1b-435a-b329-5c295067093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 14:42:04.494968: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-11 14:42:05.196556: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n",
      "2023-03-11 14:42:05.196660: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n",
      "2023-03-11 14:42:05.196665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0541e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, tfrecords_path, epochs, batch_size):\n",
    "        self.tfrecords_path = tfrecords_path\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def list_files(self):\n",
    "        return [os.path.join(tfrecords_path,file_path) for file_path in os.listdir(tfrecords_path)]\n",
    "\n",
    "    def build(self):\n",
    "        files = self.list_files()\n",
    "\n",
    "        print(\"build_tf record: files_count: {} / batch_size: {} / epochs: {}\".format(len(files), self.batch_size, self.epochs))\n",
    "\n",
    "        ds = tf.data.TFRecordDataset(files, num_parallel_reads=multiprocessing.cpu_count())\n",
    "        \n",
    "        '''''\n",
    "            Shuffle and reapeat\n",
    "        '''''\n",
    "        \n",
    "        \n",
    "        ds = ds.shuffle(buffer_size=1024 * 1 * 10)\n",
    "        ds = ds.repeat(count=self.epochs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''''\n",
    "            Map and batch\n",
    "        '''''\n",
    "        \n",
    "                      \n",
    "        ds = ds.map(self.__parse__, num_parallel_calls=None)\n",
    "        ds = ds.batch(self.batch_size,drop_remainder=False)\n",
    "        \n",
    "        \n",
    "                      \n",
    "        ds = ds.prefetch(buffer_size=5)\n",
    "                      \n",
    "\n",
    "        return ds\n",
    "    \n",
    "   \n",
    "    @staticmethod\n",
    "    def __parse__(example):\n",
    "        parsed = tf.parse_single_example(example, features={\n",
    "            'emb' : tf.io.FixedLenFeature([], tf.string),\n",
    "            'label' : tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True)\n",
    "        })\n",
    "        \n",
    "        content = tf.io.parse_single_example(element, data)\n",
    "\n",
    "        label = tf.cast(content['label'], tf.int32)\n",
    "        label_hot = tf.one_hot(label1[0], label1[1])\n",
    "        \n",
    "        emb = content['emb']\n",
    "        #get our 'feature'\n",
    "        feature = tf.io.parse_tensor(emb, out_type=tf.float32)\n",
    "\n",
    "        inp = {'emb': feature}\n",
    "        out = {'global_output': label_hot}\n",
    "\n",
    "        return inp, out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0369afd-5070-44a7-b14c-d5a58c762969",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))): # if value ist tensor\n",
    "        value = value.numpy() # get value of tensor\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a floast_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_array(array):\n",
    "  array = tf.io.serialize_tensor(array)\n",
    "  return array\n",
    "\n",
    "\n",
    "\n",
    "def parser(serialized_example):\n",
    "    features_description = {'emb': tf.io.FixedLenFeature([], tf.string)}\n",
    "    features = tf.io.parse_single_example(serialized_example, features_description)\n",
    "    features = tf.io.decode_raw(features['emb'], tf.float32)\n",
    "    return features\n",
    "\n",
    "\n",
    "def parse_tfr_element(element):\n",
    "    #use the same structure as above; it's kinda an outline of the structure we now want to create\n",
    "    data = {\n",
    "        'emb' : tf.io.FixedLenFeature([], tf.string),\n",
    "        'label' : tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True),\n",
    "    }\n",
    "    \n",
    "    content = tf.io.parse_single_example(element, data)\n",
    "\n",
    "    label = content['label']\n",
    "    emb = content['emb']\n",
    "    \n",
    "\n",
    "    #get our 'feature'-- our image -- and reshape it appropriately\n",
    "    feature = tf.io.parse_tensor(emb, out_type=tf.float32)\n",
    "    return (feature, label)\n",
    "\n",
    "\n",
    "def get_dataset(filename):\n",
    "    #create the dataset\n",
    "    dataset = tf.data.TFRecordDataset(filename)\n",
    "\n",
    "    #pass every single feature through our mapping function\n",
    "    dataset = dataset.map(\n",
    "        parse_tfr_element\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e7f8e9-f33c-47a8-9762-d5dffafd2dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = pd.Series({\n",
    "    \"root_dir\":\"/mnt/disks/data/\",\n",
    "    \"embeddings\":\"music_style\",\n",
    "    \"train_id\": \"global_sample_test\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d41cbf5-cd66-4206-8dfd-da45299b415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_path = \"/mnt/disks/data/fma/trains\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2081da1-af33-4f3e-941b-4651799ce87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "train_path = os.path.join(job_path,args.train_id)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "base_path = os.path.join(args.root_dir,\"fma\")\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "models_path = os.path.join(args.root_dir,\"models\")\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "metadata_path = os.path.join(base_path,\"fma_metadata\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55db55f5-ab4a-4325-8764-544eb506d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_dataset(train_path,dataset='train'):\n",
    "    tfrecords_path = os.path.join(train_path,'tfrecords',dataset)\n",
    "    \n",
    "    \n",
    "    tfrecords_path = [os.path.join(tfrecords_path,path) for path in os.listdir(tfrecords_path)]\n",
    "    dataset = get_dataset(tfrecords_path)\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        dataset.as_numpy_iterator(),\n",
    "        columns=['Feature', 'Label']\n",
    "    )\n",
    "        \n",
    "    df.Label = df.Label.apply(lambda x: x[0])\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        df.Feature = df.Feature.apply(lambda x: x[0] if x.shape[0] != 0 else None)\n",
    "    except:\n",
    "        print(x)\n",
    "    \n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a9f55ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 14:44:11.361434: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-11 14:44:12.108748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9164 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "df_train = load_dataset(train_path,dataset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "515f2598-79c2-4bc3-8a1c-93e2ba25efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = load_dataset(train_path,dataset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa7725da-7284-447d-a127-4d1a6a28d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = load_dataset(train_path,dataset='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4be5a17-4f7e-4934-9b38-2c388643c8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95878, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "256f03e9-c74f-4f59-ae10-b671e713a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "918b64ea-8375-4436-9952-681f53bd1e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5396, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "512cc653-38d8-4535-b902-d59cda07d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc98e5f3-c446-463e-b573-a873d8797b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "081abc05-4df9-4334-83db-093d242b72aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5128, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b48cdcae-98bd-4967-9e37-486b85e85554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1722e8ce-e1aa-4760-8626-a58eb78abbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(random_state=42,eval_metric=\"auc\",n_jobs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffc72df4-f066-4395-801c-84572e480c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all messages, including ones pertaining to debugging\n",
    "xgb.set_config(verbosity=2)\n",
    "\n",
    "# Get current value of global configuration\n",
    "# This is a dict containing all parameters in the global configuration,\n",
    "# including 'verbosity'\n",
    "config = xgb.get_config()\n",
    "assert config['verbosity'] == 2\n",
    "\n",
    "# Example of using the context manager xgb.config_context().\n",
    "# The context manager will restore the previous value of the global\n",
    "# configuration upon exiting.\n",
    "assert xgb.get_config()['verbosity'] == 2  # old value restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147a3b80-644e-4418-9bd0-3feb8ac011e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:48:06] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:08] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 120 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:12] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:13] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:15] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:17] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:19] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:21] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 26 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:22] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:24] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 116 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:26] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:28] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 120 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:30] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:31] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:33] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 124 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:35] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 16 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:37] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 124 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:39] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 118 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:40] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:42] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 46 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:44] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 104 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:46] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:48] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 98 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:50] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:52] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 122 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:54] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 104 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:55] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:57] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 106 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:48:59] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 104 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:01] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:03] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:04] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:06] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:08] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 106 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:12] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 98 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:14] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 38 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:15] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:17] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:19] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 112 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:21] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 118 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:22] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:24] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 30 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:26] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:28] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 122 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:30] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:31] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 38 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:33] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:35] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 98 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:37] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:39] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14:49:40] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 32 extra nodes, 0 pruned nodes, max_depth=6\n"
     ]
    }
   ],
   "source": [
    "xgb_model.fit(df_train.Feature.values.tolist(), df_train.Label.values.tolist(), \n",
    "        eval_set=[(df_val.Feature.values.tolist(), df_val.Label.values.tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07831e78-5949-46cd-be04-1cd2b934be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, mean_squared_error, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059c7cf-84a9-42fc-be39-25832ea6c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict(df_test.Feature.values.tolist())\n",
    "\n",
    "mean_squared_error(df_test.Label.values.tolist(), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "039ed246-d415-4838-87fd-0d9335b9a6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bruno/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bruno/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.370968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.333487</td>\n",
       "      <td>0.347911</td>\n",
       "      <td>0.337561</td>\n",
       "      <td>124.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.298337</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.317591</td>\n",
       "      <td>124.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.000000  0.000000  0.000000    1.000000\n",
       "1              0.133333  0.400000  0.200000    5.000000\n",
       "2              0.000000  0.000000  0.000000    1.000000\n",
       "3              0.000000  0.000000  0.000000    1.000000\n",
       "4              0.000000  0.000000  0.000000    1.000000\n",
       "...                 ...       ...       ...         ...\n",
       "93             1.000000  1.000000  1.000000    1.000000\n",
       "94             1.000000  1.000000  1.000000    1.000000\n",
       "accuracy       0.370968  0.370968  0.370968    0.370968\n",
       "macro avg      0.333487  0.347911  0.337561  124.000000\n",
       "weighted avg   0.298337  0.370968  0.317591  124.000000\n",
       "\n",
       "[98 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_metrics = pd.DataFrame(classification_report(df_test.Label.values.tolist(), y_pred,output_dict=True)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f50ad9-b826-4ed0-89d3-b870c2db6a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
